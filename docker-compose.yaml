version: '3.8'

services:
  vllm-qwen-2.5-coder-14b-instruct-awq:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ports:
      - "8000:8000"
    volumes:
      - "C:/models/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ:/models/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ"
    ipc: host
    environment:
      # Debug environment variables for CUDA errors
      - CUDA_LAUNCH_BLOCKING=1
      - TORCH_USE_CUDA_DSA=1
      
      # Alternative attention backends (uncomment to try different ones)
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      # - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      
      # Logging and debugging
      - VLLM_LOGGING_LEVEL=DEBUG
      
    command: [
      "--model", "/models/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
      "--served-model-name", "Qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
      "--max-model-len", "8192",
      "--kv-cache-dtype", "fp8",
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  vllm-qwen-2.5-coder-7b-instruct-awq:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ports:
      - "8000:8000"
    volumes:
      - "/mnt/c/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ/:/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ"
    ipc: host
    # environment:
    #   # Debug environment variables for CUDA errors
    #   # - CUDA_LAUNCH_BLOCKING=1
    #   # - TORCH_USE_CUDA_DSA=1
      
    #   # Alternative attention backends (uncomment to try different ones)
    #   # - VLLM_ATTENTION_BACKEND=FLASHINFER
    #   # - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      
    #   # Logging and debugging
    #   # - VLLM_LOGGING_LEVEL=DEBUG
    #   # - VLLM_USE_V1=0
      
    command: [
      "--model", "/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
      "--served-model-name", "Qwen/Qwen2.5-Coder-7B-Instruct-AWQ",
      "--max-model-len=32000", "--max-num-batched-token=16000"
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    container_name: qdrant
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    volumes:
      - "./storage:/qdrant/storage"

configs:
  qdrant_config:
    content: |
      log_level: INFO
